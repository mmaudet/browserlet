---
phase: 05-llm-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - entrypoints/background/llm/providers/types.ts
  - entrypoints/background/llm/rateLimiter.ts
  - entrypoints/background/llm/promptBuilder.ts
  - entrypoints/background/llm/fallback.ts
autonomous: true

must_haves:
  truths:
    - "Provider interface defines generateBSL and isAvailable methods"
    - "Rate limiter retries on 429 errors with exponential backoff"
    - "Prompt builder creates valid BSL generation prompt from actions"
    - "Fallback generator produces valid YAML without LLM"
  artifacts:
    - path: "entrypoints/background/llm/providers/types.ts"
      provides: "LLM provider interface and config types"
      exports: ["LLMProvider", "LLMConfig"]
    - path: "entrypoints/background/llm/rateLimiter.ts"
      provides: "Rate limiting with exponential backoff"
      exports: ["RateLimiter"]
    - path: "entrypoints/background/llm/promptBuilder.ts"
      provides: "BSL generation prompt construction"
      exports: ["buildBSLPrompt"]
    - path: "entrypoints/background/llm/fallback.ts"
      provides: "Fallback BSL generation without LLM"
      exports: ["generateBasicBSL"]
  key_links:
    - from: "entrypoints/background/llm/rateLimiter.ts"
      to: "exponential-backoff"
      via: "import { backOff }"
      pattern: "from 'exponential-backoff'"
    - from: "entrypoints/background/llm/fallback.ts"
      to: "js-yaml"
      via: "yaml.dump for output"
      pattern: "yaml\\.dump"
---

<objective>
Create LLM infrastructure types, rate limiter, prompt builder, and fallback generator.

Purpose: These are the building blocks for both Claude and Ollama providers. Rate limiting ensures API stability, prompt builder standardizes LLM input, fallback provides graceful degradation.
Output: Provider interface, rate limiter class, prompt builder function, fallback generator function.
</objective>

<execution_context>
@/Users/mmaudet/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mmaudet/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05-llm-integration/05-RESEARCH.md

# Recording types for CapturedAction
@entrypoints/content/recording/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create provider types and LLM config interface</name>
  <files>entrypoints/background/llm/providers/types.ts</files>
  <action>
Create the directory structure and types file:
mkdir -p entrypoints/background/llm/providers

Define the LLMProvider interface with:
- name: 'claude' | 'ollama'
- generateBSL(actions: CapturedAction[]): Promise<string>
- isAvailable(): Promise<boolean>

Define LLMConfig interface with:
- provider: 'claude' | 'ollama'
- claudeApiKey?: string (encrypted in storage)
- claudeModel?: string (default: 'claude-sonnet-4-5-20250929')
- ollamaHost?: string (default: 'http://localhost:11434')
- ollamaModel?: string (default: 'llama3.1')

Import CapturedAction from '../../../../entrypoints/content/recording/types'.

Export both interfaces.
  </action>
  <verify>TypeScript compiles without errors: npx tsc --noEmit entrypoints/background/llm/providers/types.ts</verify>
  <done>types.ts exports LLMProvider interface and LLMConfig interface</done>
</task>

<task type="auto">
  <name>Task 2: Create rate limiter with exponential backoff</name>
  <files>entrypoints/background/llm/rateLimiter.ts</files>
  <action>
Create rate limiter class using exponential-backoff library.

Import { backOff } from 'exponential-backoff'.

Define RateLimitState interface:
- lastRequest: number
- retryAfter: number | null
- consecutiveErrors: number

Create RateLimiter class with:
- private state: RateLimitState
- execute<T>(fn: () => Promise<T>): Promise<T>

The execute method:
1. Check if currently rate limited (retryAfter > Date.now())
2. If rate limited, throw error with remaining wait time
3. Use backOff() to wrap the function with:
   - numOfAttempts: 5
   - startingDelay: 1000
   - timeMultiple: 2
   - maxDelay: 30000
   - jitter: 'full' (add randomness to avoid thundering herd)
   - retry: (error) => error.status === 429 (only retry rate limit errors)
4. On 429 error, parse retry-after header if present and update state
5. On success, reset consecutiveErrors and retryAfter
6. Non-429 errors are thrown immediately without retry

Export the RateLimiter class.
  </action>
  <verify>npm run build succeeds (file compiles without errors)</verify>
  <done>rateLimiter.ts exports RateLimiter class with execute method</done>
</task>

<task type="auto">
  <name>Task 3: Create prompt builder and fallback generator</name>
  <files>entrypoints/background/llm/promptBuilder.ts, entrypoints/background/llm/fallback.ts</files>
  <action>
Create two files:

**promptBuilder.ts:**
Import CapturedAction from content/recording/types.

Export function buildBSLPrompt(actions: CapturedAction[]): string

Build a structured prompt with:
1. System context: "You are a BSL expert..."
2. BSL format specification (YAML structure with name, version, description, steps)
3. Hint types list with reliability ordering:
   - data_attribute (most stable)
   - role, type, aria_label, name
   - text_contains, placeholder_contains
   - near_label, class_contains, id (least reliable unless stable)
4. Rules:
   - Include 2-3 hints per target
   - Put most reliable hints first
   - Use semantic intent descriptions
   - Add fallback_selector only for unique IDs
   - Use wait_for before dynamic elements
5. JSON-serialized actions array
6. Instruction: "Output ONLY the YAML, no explanations"

**fallback.ts:**
Import CapturedAction and yaml from js-yaml.

Export function generateBasicBSL(actions: CapturedAction[]): string

Map captured actions to BSL steps:
1. Map action.type to BSL action type (click->click, input->type, navigate->navigate, submit->click)
2. Use top 3 hints from action.hints
3. Create intent as "Step N: {action.type} action"
4. Include value field for input actions
5. Include targetUrl for navigate actions

Build script object:
```typescript
{
  name: 'Recorded Script',
  version: '1.0.0',
  description: 'Auto-generated from recording (basic mode)',
  steps: mappedSteps
}
```

Use yaml.dump with options: { indent: 2, lineWidth: -1, noRefs: true }
  </action>
  <verify>npm run build succeeds (both files compile without errors)</verify>
  <done>promptBuilder.ts exports buildBSLPrompt, fallback.ts exports generateBasicBSL</done>
</task>

</tasks>

<verification>
1. npm run build completes without errors
2. All four files exist in entrypoints/background/llm/
3. Directory structure: llm/providers/types.ts, llm/rateLimiter.ts, llm/promptBuilder.ts, llm/fallback.ts
</verification>

<success_criteria>
- LLMProvider interface matches research specification
- RateLimiter uses exponential-backoff with jitter
- Prompt includes all 10 hint types with reliability ordering
- Fallback generates valid YAML using js-yaml
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-integration/05-02-SUMMARY.md`
</output>
