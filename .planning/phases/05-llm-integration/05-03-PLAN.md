---
phase: 05-llm-integration
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - entrypoints/background/llm/providers/claude.ts
  - entrypoints/background/llm/providers/ollama.ts
  - entrypoints/background/llm/index.ts
autonomous: true
user_setup:
  - service: anthropic
    why: "Claude API for BSL generation"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/account/keys)"
    dashboard_config: []
  - service: ollama
    why: "Local LLM alternative"
    env_vars: []
    dashboard_config:
      - task: "Install and run Ollama"
        location: "https://ollama.com/download - install app, run 'ollama pull llama3.1'"

must_haves:
  truths:
    - "Claude provider sends requests to api.anthropic.com"
    - "Ollama provider connects to localhost:11434"
    - "LLM service falls back to basic generation on provider failure"
    - "LLM service is a singleton accessible from service worker"
  artifacts:
    - path: "entrypoints/background/llm/providers/claude.ts"
      provides: "Claude API integration"
      exports: ["ClaudeProvider"]
    - path: "entrypoints/background/llm/providers/ollama.ts"
      provides: "Ollama local LLM integration"
      exports: ["OllamaProvider"]
    - path: "entrypoints/background/llm/index.ts"
      provides: "LLM service facade with provider abstraction"
      exports: ["LLMService", "getLLMService"]
  key_links:
    - from: "entrypoints/background/llm/providers/claude.ts"
      to: "@anthropic-ai/sdk"
      via: "import Anthropic"
      pattern: "from '@anthropic-ai/sdk'"
    - from: "entrypoints/background/llm/providers/ollama.ts"
      to: "ollama/browser"
      via: "import { Ollama }"
      pattern: "from 'ollama/browser'"
    - from: "entrypoints/background/llm/index.ts"
      to: "entrypoints/background/llm/fallback.ts"
      via: "fallbackGeneration on error"
      pattern: "generateBasicBSL"
---

<objective>
Implement Claude and Ollama providers, and create the LLM service facade.

Purpose: The providers wrap the official SDKs with rate limiting and error handling. The LLM service provides a unified interface that automatically falls back to basic generation when providers fail.
Output: Working Claude provider, Ollama provider, and LLM service singleton.
</objective>

<execution_context>
@/Users/mmaudet/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mmaudet/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/05-llm-integration/05-RESEARCH.md

# Dependencies from previous plans
@entrypoints/background/llm/providers/types.ts
@entrypoints/background/llm/rateLimiter.ts
@entrypoints/background/llm/promptBuilder.ts
@entrypoints/background/llm/fallback.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Claude provider</name>
  <files>entrypoints/background/llm/providers/claude.ts</files>
  <action>
Create Claude provider implementing LLMProvider interface.

Import:
- Anthropic from '@anthropic-ai/sdk'
- yaml from 'js-yaml'
- RateLimiter from '../rateLimiter'
- buildBSLPrompt from '../promptBuilder'
- LLMProvider, LLMConfig from './types'
- CapturedAction from content/recording/types

Create ClaudeProvider class:
- name = 'claude' as const
- private client: Anthropic
- private rateLimiter: RateLimiter
- private model: string

Constructor(apiKey: string, model?: string):
- Initialize Anthropic client with apiKey
- Create new RateLimiter instance
- Set model with default 'claude-sonnet-4-5-20250929'

isAvailable(): Promise<boolean>
- Quick health check with minimal tokens
- Use rateLimiter.execute to wrap client.messages.create
- Model: 'claude-haiku-4-5-20250929' (cheapest for health check)
- max_tokens: 1, messages: [{ role: 'user', content: 'hi' }]
- Return true on success, false on any error

generateBSL(actions: CapturedAction[]): Promise<string>
- Build prompt using buildBSLPrompt(actions)
- Use rateLimiter.execute to wrap API call
- model: this.model, max_tokens: 4096
- Extract text content from response.content array
- Validate YAML with yaml.load() - throws if invalid
- Return the BSL string
- On parse error, throw Error('LLM generated invalid BSL')

Export ClaudeProvider class.
  </action>
  <verify>npm run build succeeds (file compiles without type errors)</verify>
  <done>ClaudeProvider implements LLMProvider with generateBSL and isAvailable</done>
</task>

<task type="auto">
  <name>Task 2: Implement Ollama provider</name>
  <files>entrypoints/background/llm/providers/ollama.ts</files>
  <action>
Create Ollama provider implementing LLMProvider interface.

Import:
- { Ollama } from 'ollama/browser'
- yaml from 'js-yaml'
- buildBSLPrompt from '../promptBuilder'
- LLMProvider from './types'
- CapturedAction from content/recording/types

Note: Ollama doesn't need rate limiting - it's local with no API limits.

Create OllamaProvider class:
- name = 'ollama' as const
- private client: Ollama
- private model: string

Constructor(host?: string, model?: string):
- host defaults to 'http://localhost:11434'
- model defaults to 'llama3.1'
- Initialize Ollama client with { host }
- Store model

isAvailable(): Promise<boolean>
- Try await this.client.list() to check Ollama is running
- Return true on success, false on any error (connection refused, etc.)

generateBSL(actions: CapturedAction[]): Promise<string>
- Build prompt using buildBSLPrompt(actions)
- Call client.chat with:
  - model: this.model
  - messages: [{ role: 'user', content: prompt }]
  - stream: false
- Extract response.message.content
- Validate YAML with yaml.load()
- Return BSL string
- On parse error, throw Error('LLM generated invalid BSL')

Export OllamaProvider class.
  </action>
  <verify>npm run build succeeds (file compiles without type errors)</verify>
  <done>OllamaProvider implements LLMProvider with generateBSL and isAvailable</done>
</task>

<task type="auto">
  <name>Task 3: Create LLM service facade</name>
  <files>entrypoints/background/llm/index.ts</files>
  <action>
Create the LLM service facade that provides provider abstraction and fallback.

Import:
- LLMProvider, LLMConfig from './providers/types'
- ClaudeProvider from './providers/claude'
- OllamaProvider from './providers/ollama'
- generateBasicBSL from './fallback'
- CapturedAction from content/recording/types

Create LLMService class:
- private provider: LLMProvider | null = null
- private config: LLMConfig | null = null
- private initialized: boolean = false

async initialize(config: LLMConfig): Promise<void>
- Store config
- If provider is 'claude' AND claudeApiKey exists:
  - Create new ClaudeProvider(config.claudeApiKey, config.claudeModel)
- Else if provider is 'ollama':
  - Create new OllamaProvider(config.ollamaHost, config.ollamaModel)
- Set initialized = true

async generateBSL(actions: CapturedAction[]): Promise<{ bsl: string; usedLLM: boolean }>
- If no provider or not initialized:
  - Return { bsl: generateBasicBSL(actions), usedLLM: false }
- Try:
  - Check provider.isAvailable()
  - If not available: return { bsl: generateBasicBSL(actions), usedLLM: false }
  - Call provider.generateBSL(actions)
  - Return { bsl: result, usedLLM: true }
- Catch any error:
  - console.error('[LLM] Provider failed, using fallback:', error)
  - Return { bsl: generateBasicBSL(actions), usedLLM: false }

getStatus(): { provider: string | null; configured: boolean; initialized: boolean }
- Return current provider name (or null), whether API key exists, and init state

isConfigured(): boolean
- Check if config exists and has required credentials for chosen provider

Create singleton pattern:
- let instance: LLMService | null = null
- export function getLLMService(): LLMService
  - If !instance, create new LLMService()
  - Return instance

Export LLMService class and getLLMService function.
  </action>
  <verify>npm run build succeeds (all LLM files compile together)</verify>
  <done>index.ts exports LLMService class with initialize, generateBSL, getStatus methods plus getLLMService singleton</done>
</task>

</tasks>

<verification>
1. npm run build completes without errors
2. All three provider files exist and export correctly
3. LLMService provides fallback when no provider configured
4. Directory structure complete: llm/index.ts, llm/providers/{types,claude,ollama}.ts
</verification>

<success_criteria>
- ClaudeProvider uses @anthropic-ai/sdk with rate limiting
- OllamaProvider uses ollama/browser package
- LLMService falls back to generateBasicBSL on any failure
- getLLMService returns singleton instance
- All files type-check correctly
</success_criteria>

<output>
After completion, create `.planning/phases/05-llm-integration/05-03-SUMMARY.md`
</output>
